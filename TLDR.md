## Task 1: Checking for Bias in NLP (BERT):
TLDR: An experiment using sentiment analysis on sentences with identity terms revealed significant bias in the NLP model. The model displayed discrimination against certain groups, especially when it came to religious and geographical identities. The bias was quantified and ordered for different categories. In terms of religious bias, the model showed the most negative sentiment towards Muslim people, followed by Sikh, Buddhist, and Hindu individuals. For geographical bias, the model exhibited negative bias against people from North-Eastern states, followed by Northern states, with a slight bias towards South Indian states. When analyzing caste bias, the order of bias was Vaisya, Sudra, Kshatriya, and Brahmin, indicating sporadic bias towards certain castes.

## Task 2: Analyzing Prompts and Replies for Fairness in Legal AI:
TLDR: The prompts in the dataset appear to be programmatically generated using a Python script, incorporating Indian names, religions, and sexes. The structure of the prompts includes detailed legal descriptions and identity terms related to geography, religion, and caste. While the geographical terms are arranged alphabetically, there is a lack of consistent order for religious and caste terms. The use of caste names as identifiers may lead to biased data due to their association with hate speech. It is suggested to use caste-indicative last names instead, as these are more frequently used in text and may reduce bias. The dataset aims for equal representation of male and female accused. Some prompts also include identity terms for victims, adding complexity to the analysis.

## Bonus Task: Bias Analysis in LLM Output:
TLDR: The LLM exhibited a notable bias towards false positives, with a high rate of 50.37%. Interestingly, there was a strong tendency to predict offenses related to sexual harassment and its variants as false negatives. When analyzing bias in false positives, the model showed bias towards males, with higher numbers for Brahmin, Kshatriya, and Shudra individuals. In terms of religious bias, Buddhist, Sikh, and Christian people were more likely to be falsely accused. Conversely, in false negatives, the model showed bias towards females, with the majority of false negatives being associated with male accused. For religious bias in false negatives, Buddhist, Sikh, and Muslim people were more likely to be incorrectly cleared, while Christian accused were never falsely cleared. The model's bias towards specific social groups is concerning and warrants further investigation.

## Footnotes
This TL;DR was generated by AI, specifically, Cohere's command r+ model.
In the bonus task, due to the complexity of the geographical identity terms, I was not able to analyze them. Also, the analysis was only performed on one of the datasets (alpha) due to time constraints.
After drafting the document, I ran it through Grammarly to catch any grammatical errors and typos.
Images used for proof for things mentioned are provided in the images folder, it contains screenshots of the Google n-gram viewer and the data from analyzing NLP and LLM arranged in tables and graphs.